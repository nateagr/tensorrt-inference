{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compile CLIP text model, I had to convert input text ids to float before calling argmax()\n",
    "# Otherwise ONNX parsing fails due to \"This version of TensorRT does not support INT32 ArgMin/ArgMax input data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2bb5c5bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:04:44.523228Z",
     "start_time": "2022-01-18T08:04:44.519596Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import clip\n",
    "import onnx\n",
    "from onnxruntime import InferenceSession\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "\n",
    "from tensorrt_inference.backend import (\n",
    "    build_engine, save_engine, load_engine\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffb5f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:18.587764Z",
     "start_time": "2022-01-18T07:45:12.581285Z"
    }
   },
   "outputs": [],
   "source": [
    "model, _ = clip.load(\"ViT-B/32\")\n",
    "model = model.eval()\n",
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc925b20",
   "metadata": {},
   "source": [
    "## Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c33e90f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:21.299069Z",
     "start_time": "2022-01-18T07:45:21.282910Z"
    }
   },
   "outputs": [],
   "source": [
    "image_tensors = torch.stack([torch.rand((3, 224, 224))]).to(\"cuda:0\")\n",
    "text_tensors = torch.stack([torch.randint(0, 100, (77,), dtype=torch.int32)]).to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1e980b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:21.647960Z",
     "start_time": "2022-01-18T07:45:21.580184Z"
    }
   },
   "outputs": [],
   "source": [
    "image_embeddings = model.visual(image_tensors.half())\n",
    "text_embeddings = model(text_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897a510b",
   "metadata": {},
   "source": [
    "## ONNX export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a67c4462",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:23.073506Z",
     "start_time": "2022-01-18T07:45:23.067919Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_onnx(\n",
    "    model_pytorch, output_path: str, inputs_pytorch: Dict[str, torch.Tensor], quantization: bool\n",
    ") -> None:\n",
    "    if quantization:\n",
    "        try:\n",
    "            from pytorch_quantization.nn import TensorQuantizer\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"It seems that pytorch-quantization is not yet installed. \"\n",
    "                \"It is required when you enable the quantization flag and use CUDA device.\"\n",
    "                \"Please find installation instructions on \"\n",
    "                \"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization or use:\\n\"\n",
    "                \"pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization\\\\&\"\n",
    "                \"subdirectory=tools/pytorch-quantization/\"\n",
    "            )\n",
    "\n",
    "        TensorQuantizer.use_fb_fake_quant = True\n",
    "\n",
    "    dynamic_axis = dict()\n",
    "    for k in inputs_pytorch.keys():\n",
    "        dynamic_axis[k] = {0: \"batch_size\", 1: \"sequence\"}\n",
    "    dynamic_axis[\"output\"] = {0: \"batch_size\"}\n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            model_pytorch,\n",
    "            args=tuple(inputs_pytorch.values()),\n",
    "            f=output_path,\n",
    "            opset_version=12,\n",
    "            do_constant_folding=True,\n",
    "            input_names=list(inputs_pytorch.keys()),\n",
    "            output_names=[\"output\"],\n",
    "            dynamic_axes=dynamic_axis,\n",
    "            training=torch.onnx.TrainingMode.EVAL,\n",
    "            verbose=False,\n",
    "        )\n",
    "    if quantization:\n",
    "        TensorQuantizer.use_fb_fake_quant = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190d591b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:23.655897Z",
     "start_time": "2022-01-18T07:45:23.654067Z"
    }
   },
   "outputs": [],
   "source": [
    "clip_vit_onnx_path = \"/home/g.racic/clip_vit_onnx.onnx\"\n",
    "clip_transformer_onnx_path = \"/home/g.racic/clip_transformer_onnx.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dbce3a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:23.950662Z",
     "start_time": "2022-01-18T07:45:23.948671Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_image = {\"image\": image_tensors.half()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "551f7187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:28.647119Z",
     "start_time": "2022-01-18T07:45:24.492126Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0118 07:45:24.499392 140626572773184 amp_wrapper.py:31] AMP is not avaialble.\n"
     ]
    }
   ],
   "source": [
    "convert_to_onnx(model.visual, clip_vit_onnx_path, sample_image, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c4281e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:28.651454Z",
     "start_time": "2022-01-18T07:45:28.649299Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_text = {\"text\": text_tensors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3528c6d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:33.026937Z",
     "start_time": "2022-01-18T07:45:28.652649Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/home/g.racic/tensorrt_env/lib64/python3.6/site-packages/torch/onnx/symbolic_opset9.py:2819: UserWarning: Exporting aten::index operator of advanced indexing in opset 12 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  \"If indices include negative values, the exported graph will produce incorrect results.\")\n"
     ]
    }
   ],
   "source": [
    "convert_to_onnx(model, clip_transformer_onnx_path, sample_text, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318523e7",
   "metadata": {},
   "source": [
    "## ONNX inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2cb15ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:33.311333Z",
     "start_time": "2022-01-18T07:45:33.028434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model expects input shape:  ['batch_size', 'sequence', 224, 224]\n"
     ]
    }
   ],
   "source": [
    "sess_vit = InferenceSession(clip_vit_onnx_path)\n",
    "print(\"The model expects input shape: \", sess_vit.get_inputs()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5471dd0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:19:56.531566Z",
     "start_time": "2022-01-18T07:19:56.322729Z"
    }
   },
   "outputs": [],
   "source": [
    "sess_vit.run(None, {\"image\": image_tensors.half().cpu().numpy()})\n",
    "# 150 ms ± 456 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf060b7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:35.179465Z",
     "start_time": "2022-01-18T07:45:34.840625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model expects input shape:  ['batch_size', 'sequence']\n"
     ]
    }
   ],
   "source": [
    "sess_transformer = InferenceSession(clip_transformer_onnx_path)\n",
    "print(\"The model expects input shape: \", sess_transformer.get_inputs()[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093d149",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:20:00.749855Z",
     "start_time": "2022-01-18T07:20:00.633335Z"
    }
   },
   "outputs": [],
   "source": [
    "sess_transformer.run(None, {\"text\": text_tensors.cpu().numpy()})\n",
    "# 82.5 ms ± 515 µs per loop (mean ± std. dev. of 7 runs, 20 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b2047e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:45:36.895244Z",
     "start_time": "2022-01-18T07:45:36.781223Z"
    }
   },
   "outputs": [],
   "source": [
    "model_proto = onnx.load(clip_transformer_onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9e5f2d",
   "metadata": {},
   "source": [
    "## TRT compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "035ebdcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:12:22.016423Z",
     "start_time": "2022-01-18T08:12:22.013040Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_logger = trt.Logger(trt.Logger.VERBOSE)\n",
    "runtime = trt.Runtime(trt_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d4898dc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:12:22.171661Z",
     "start_time": "2022-01-18T08:12:22.168978Z"
    }
   },
   "outputs": [],
   "source": [
    "# Min, optim, max shapes used for TRT optimizer\n",
    "text_tensor_shapes = [(1, 77), (1, 77), (1, 77)]\n",
    "image_tensor_shapes = [(1, 3, 224, 224), (1, 3, 224, 224), (1, 3, 224, 224)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32c34518",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:59:58.670095Z",
     "start_time": "2022-01-18T07:59:29.851600Z"
    }
   },
   "outputs": [],
   "source": [
    "text_engine = build_engine(\n",
    "    runtime=runtime,\n",
    "    onnx_file_path=clip_transformer_onnx_path,\n",
    "    logger=trt_logger,\n",
    "    min_shape=text_tensor_shapes[0],\n",
    "    optimal_shape=text_tensor_shapes[1],\n",
    "    max_shape=text_tensor_shapes[2],\n",
    "    workspace_size=10000 * 1024 * 1024,\n",
    "    fp16=True,\n",
    "    int8=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "058ccaa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:59:58.673374Z",
     "start_time": "2022-01-18T07:59:58.671471Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_text_clip_path = \"/home/g.racic/trt_text_clip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b699f9b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:59:58.871370Z",
     "start_time": "2022-01-18T07:59:58.674535Z"
    }
   },
   "outputs": [],
   "source": [
    "save_engine(engine=engine, engine_file_path=trt_text_clip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "630a07ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T07:59:59.200435Z",
     "start_time": "2022-01-18T07:59:58.872637Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_text_model = load_engine(\n",
    "    runtime=runtime, engine_file_path=trt_text_clip_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "41a37cdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:13:29.243891Z",
     "start_time": "2022-01-18T08:12:45.114734Z"
    }
   },
   "outputs": [],
   "source": [
    "image_engine = build_engine(\n",
    "    runtime=runtime,\n",
    "    onnx_file_path=clip_vit_onnx_path,\n",
    "    logger=trt_logger,\n",
    "    min_shape=image_tensor_shapes[0],\n",
    "    optimal_shape=image_tensor_shapes[1],\n",
    "    max_shape=image_tensor_shapes[2],\n",
    "    workspace_size=10000 * 1024 * 1024,\n",
    "    fp16=True,\n",
    "    int8=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e46503d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:13:29.248093Z",
     "start_time": "2022-01-18T08:13:29.245791Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_image_clip_path = \"/home/g.racic/trt_image_clip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1aeed635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:13:29.752422Z",
     "start_time": "2022-01-18T08:13:29.249349Z"
    }
   },
   "outputs": [],
   "source": [
    "save_engine(engine=engine, engine_file_path=trt_image_clip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b45e7989",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:13:30.086278Z",
     "start_time": "2022-01-18T08:13:29.754017Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_image_model = load_engine(\n",
    "    runtime=runtime, engine_file_path=trt_image_clip_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1008d50",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef06a431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:03:37.435745Z",
     "start_time": "2022-01-18T08:03:37.430513Z"
    }
   },
   "outputs": [],
   "source": [
    "def benchmark(model_fn, input_data, batch_size, nwarmup=50, nruns=1000):\n",
    "    _data = itertools.cycle(input_data)\n",
    "    print(\"Warm up ...\")\n",
    "    with torch.no_grad():\n",
    "        for n in range(nwarmup):\n",
    "            model_fn(next(_data))\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, nruns+1):\n",
    "            start_time = time.time()\n",
    "            model_fn(next(_data))\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            timings.append(end_time - start_time)\n",
    "            if i%100==0:\n",
    "                print('Iteration %d/%d, avg batch time %.2f ms'%(i, nruns, np.mean(timings)*1000))\n",
    " \n",
    "    print('Average throughput: %.2f example/second'%(batch_size/np.mean(timings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "149f154c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:04:48.338831Z",
     "start_time": "2022-01-18T08:04:46.794297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 100/1000, avg batch time 1.52 ms\n",
      "Iteration 200/1000, avg batch time 1.49 ms\n",
      "Iteration 300/1000, avg batch time 1.48 ms\n",
      "Iteration 400/1000, avg batch time 1.47 ms\n",
      "Iteration 500/1000, avg batch time 1.47 ms\n",
      "Iteration 600/1000, avg batch time 1.47 ms\n",
      "Iteration 700/1000, avg batch time 1.46 ms\n",
      "Iteration 800/1000, avg batch time 1.46 ms\n",
      "Iteration 900/1000, avg batch time 1.46 ms\n",
      "Iteration 1000/1000, avg batch time 1.46 ms\n",
      "Average throughput: 684.90 example/second\n"
     ]
    }
   ],
   "source": [
    "benchmark(trt_image_model, [{\"image\": image_tensors.half().cpu().numpy()}], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "06d8bfd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:05:14.601911Z",
     "start_time": "2022-01-18T08:05:06.725344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 100/1000, avg batch time 7.48 ms\n",
      "Iteration 200/1000, avg batch time 7.49 ms\n",
      "Iteration 300/1000, avg batch time 7.49 ms\n",
      "Iteration 400/1000, avg batch time 7.49 ms\n",
      "Iteration 500/1000, avg batch time 7.49 ms\n",
      "Iteration 600/1000, avg batch time 7.49 ms\n",
      "Iteration 700/1000, avg batch time 7.49 ms\n",
      "Iteration 800/1000, avg batch time 7.49 ms\n",
      "Iteration 900/1000, avg batch time 7.49 ms\n",
      "Iteration 1000/1000, avg batch time 7.49 ms\n",
      "Average throughput: 133.56 example/second\n"
     ]
    }
   ],
   "source": [
    "benchmark(model.visual, [image_tensors.half()], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1f8a033e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:05:37.125953Z",
     "start_time": "2022-01-18T08:05:35.676801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 100/1000, avg batch time 1.47 ms\n",
      "Iteration 200/1000, avg batch time 1.42 ms\n",
      "Iteration 300/1000, avg batch time 1.40 ms\n",
      "Iteration 400/1000, avg batch time 1.39 ms\n",
      "Iteration 500/1000, avg batch time 1.39 ms\n",
      "Iteration 600/1000, avg batch time 1.38 ms\n",
      "Iteration 700/1000, avg batch time 1.38 ms\n",
      "Iteration 800/1000, avg batch time 1.37 ms\n",
      "Iteration 900/1000, avg batch time 1.37 ms\n",
      "Iteration 1000/1000, avg batch time 1.37 ms\n",
      "Average throughput: 729.70 example/second\n"
     ]
    }
   ],
   "source": [
    "benchmark(trt_text_model, [{\"text\": text_tensors.cpu().numpy()}], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06e5ca83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-18T08:06:00.032968Z",
     "start_time": "2022-01-18T08:05:52.237939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 100/1000, avg batch time 7.39 ms\n",
      "Iteration 200/1000, avg batch time 7.39 ms\n",
      "Iteration 300/1000, avg batch time 7.40 ms\n",
      "Iteration 400/1000, avg batch time 7.40 ms\n",
      "Iteration 500/1000, avg batch time 7.40 ms\n",
      "Iteration 600/1000, avg batch time 7.41 ms\n",
      "Iteration 700/1000, avg batch time 7.41 ms\n",
      "Iteration 800/1000, avg batch time 7.41 ms\n",
      "Iteration 900/1000, avg batch time 7.41 ms\n",
      "Iteration 1000/1000, avg batch time 7.41 ms\n",
      "Average throughput: 134.95 example/second\n"
     ]
    }
   ],
   "source": [
    "benchmark(model, [text_tensors], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd77b52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "tensorrt_env",
   "language": "python",
   "name": "tensorrt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
