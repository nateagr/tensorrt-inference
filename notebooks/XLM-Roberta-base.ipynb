{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fcb9d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:02.095618Z",
     "start_time": "2022-03-03T11:26:00.476496Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/nfs/home/g.racic/tensorrt_env_2/lib64/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, AutoModel\n",
    "from transformers import XLMRobertaTokenizerFast\n",
    "import onnx\n",
    "import tensorrt as trt\n",
    "\n",
    "from tensorrt_inference.backend import (\n",
    "    build_engine, save_engine, load_engine\n",
    ")\n",
    "from tensorrt_inference.benchmark import (\n",
    "    generate_random_input_for_transformers, run_inference, compute_mean_discrepency\n",
    ")\n",
    "from tensorrt_inference.transformers import MinMaxCalibratorTransformers\n",
    "from tensorrt_inference.onnx import convert_to_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db75bbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:02.105421Z",
     "start_time": "2022-03-03T11:26:02.097419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/g.racic'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home_dir = str(Path.home())\n",
    "home_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63ab30",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d9171d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:03.218610Z",
     "start_time": "2022-03-03T11:26:03.214656Z"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"_name_or_path\": \"xlm-roberta-base\",\n",
    "  \"architectures\": [\n",
    "    \"XLMRobertaForMaskedLM\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"classifier_dropout\": None,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"xlm-roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"output_past\": True,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.15.0\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_cache\": True,\n",
    "  \"vocab_size\": 250002\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "996c087c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:03.668535Z",
     "start_time": "2022-03-03T11:26:03.664904Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"w\") as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba4ac4e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:04.328948Z",
     "start_time": "2022-03-03T11:26:04.324792Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_categories):\n",
    "        super().__init__()\n",
    "        self.n_categories = n_categories\n",
    "        config = AutoConfig.from_pretrained(\"config.json\")\n",
    "        self.base_model = AutoModel.from_config(config)\n",
    "        self.category_embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=self.n_categories, embedding_dim=768\n",
    "        )\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.FloatTensor,\n",
    "        attention_mask: torch.FloatTensor,\n",
    "    ):\n",
    "        text_embedding = self.base_model(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        ).pooler_output\n",
    "        return text_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8987fc8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:08.177567Z",
     "start_time": "2022-03-03T11:26:04.883824Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (base_model): XLMRobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (category_embeddings): Embedding(233, 768)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(233)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b6e01",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf28585f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:08.180856Z",
     "start_time": "2022-03-03T11:26:08.178976Z"
    }
   },
   "outputs": [],
   "source": [
    "seq_max_length = 96\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f79a666",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:11.819665Z",
     "start_time": "2022-03-03T11:26:08.182104Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe1ef7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:11.823480Z",
     "start_time": "2022-03-03T11:26:11.821045Z"
    }
   },
   "outputs": [],
   "source": [
    "data = \\\n",
    "\"\"\"\n",
    "Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's\n",
    "standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make\n",
    "a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting,\n",
    "remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing\n",
    "Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions\n",
    "of Lorem Ipsum. Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of\n",
    "classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at\n",
    "Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem\n",
    "Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source.\n",
    "Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and\n",
    "Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the \n",
    "Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.\n",
    "The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. \n",
    "Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their \n",
    "exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "460f8c05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:11.831060Z",
     "start_time": "2022-03-03T11:26:11.824441Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer([data], max_length=seq_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa0a787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:16.927906Z",
     "start_time": "2022-03-03T11:26:11.832353Z"
    }
   },
   "outputs": [],
   "source": [
    "random_tensor_inputs, random_numpy_inputs = generate_random_input_for_transformers(\n",
    "    n_inputs=10,\n",
    "    batch_size=batch_size,\n",
    "    seq_len=seq_max_length,\n",
    "    include_token_ids=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cce08b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:16.932833Z",
     "start_time": "2022-03-03T11:26:16.929726Z"
    }
   },
   "outputs": [],
   "source": [
    "random_tensor_inputs_cpu = [\n",
    "    {k: v.to(\"cpu\") for k, v in tensor_input.items()}\n",
    "    for tensor_input in random_tensor_inputs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5ac6344",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:41.146392Z",
     "start_time": "2022-03-03T11:26:16.933800Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pytorch_cpu_outputs, pytorch_cpu_time_buffer = run_inference(\n",
    "        inference_fn=lambda x: model(**x), inputs=random_tensor_inputs_cpu, n_measures=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1248925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:41.415191Z",
     "start_time": "2022-03-03T11:26:41.148261Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "795e8c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:44.768833Z",
     "start_time": "2022-03-03T11:26:41.417069Z"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pytorch_gpu_outputs, pytorch_gpu_time_buffer = run_inference(\n",
    "        inference_fn=lambda x: model(**x), inputs=random_tensor_inputs, n_measures=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee79a6",
   "metadata": {},
   "source": [
    "## Convert model to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f30c0158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:44.773503Z",
     "start_time": "2022-03-03T11:26:44.770482Z"
    }
   },
   "outputs": [],
   "source": [
    "onnx_model_path = os.path.join(home_dir, \"onnx-xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9eed34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:54.100357Z",
     "start_time": "2022-03-03T11:26:44.774993Z"
    }
   },
   "outputs": [],
   "source": [
    "convert_to_onnx(model.to(\"cpu\"), onnx_model_path, tokens, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77aefb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T07:40:07.968131Z",
     "start_time": "2022-03-03T07:40:07.816353Z"
    }
   },
   "source": [
    "## FP-16 quantization with TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13ec946b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:54.104506Z",
     "start_time": "2022-03-03T11:26:54.102074Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_model_path = os.path.join(home_dir, \"trt-xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae7e4f4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:26:54.348808Z",
     "start_time": "2022-03-03T11:26:54.106737Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_logger = trt.Logger(trt.Logger.INFO)\n",
    "runtime = trt.Runtime(trt_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88a75607",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:27:39.930283Z",
     "start_time": "2022-03-03T11:26:54.350514Z"
    }
   },
   "outputs": [],
   "source": [
    "engine_fp16 = build_engine(\n",
    "    runtime=runtime,\n",
    "    onnx_file_path=onnx_model_path,\n",
    "    logger=trt_logger,\n",
    "    min_shape=(batch_size, seq_max_length),\n",
    "    optimal_shape=(batch_size, seq_max_length),\n",
    "    max_shape=(batch_size, seq_max_length),\n",
    "    workspace_size=10000 * 1024 * 1024,\n",
    "    fp16=True,\n",
    "    int8=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de2206db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:27:40.871481Z",
     "start_time": "2022-03-03T11:27:39.932153Z"
    }
   },
   "outputs": [],
   "source": [
    "save_engine(engine=engine_fp16, engine_file_path=trt_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bdd70fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:27:41.382154Z",
     "start_time": "2022-03-03T11:27:40.873102Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_fp16_model = load_engine(\n",
    "    runtime=runtime, engine_file_path=trt_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12dd3896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:27:42.081854Z",
     "start_time": "2022-03-03T11:27:41.383382Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_fp16_outputs, trt_fp16_time_buffer = run_inference(\n",
    "    inference_fn=trt_fp16_model, inputs=random_numpy_inputs, n_measures=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1873946",
   "metadata": {},
   "source": [
    "## INT-8 quantization with TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2df44f08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:27:42.085043Z",
     "start_time": "2022-03-03T11:27:42.082977Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_int8_model_path = os.path.join(home_dir, \"trt-int8-xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6461b1d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:44.951906Z",
     "start_time": "2022-03-03T11:27:42.086062Z"
    }
   },
   "outputs": [],
   "source": [
    "engine_int8 = build_engine(\n",
    "    runtime=runtime,\n",
    "    onnx_file_path=onnx_model_path,\n",
    "    logger=trt_logger,\n",
    "    min_shape=(batch_size, seq_max_length),\n",
    "    optimal_shape=(batch_size, seq_max_length),\n",
    "    max_shape=(batch_size, seq_max_length),\n",
    "    workspace_size=10000 * 1024 * 1024,\n",
    "    fp16=True,\n",
    "    int8=True,\n",
    "    calibrator=MinMaxCalibratorTransformers(random_numpy_inputs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "29edfef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:45.817063Z",
     "start_time": "2022-03-03T11:29:44.953734Z"
    }
   },
   "outputs": [],
   "source": [
    "save_engine(engine=engine_int8, engine_file_path=trt_int8_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d05bdf19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:46.322330Z",
     "start_time": "2022-03-03T11:29:45.818715Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_int8_model = load_engine(\n",
    "    runtime=runtime, engine_file_path=trt_int8_model_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9ecdc66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:47.022082Z",
     "start_time": "2022-03-03T11:29:46.323534Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_int8_outputs, trt_int8_time_buffer = run_inference(\n",
    "    inference_fn=trt_int8_model, inputs=random_numpy_inputs, n_measures=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3098a33b",
   "metadata": {},
   "source": [
    "## Output comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3f6d0c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:47.026968Z",
     "start_time": "2022-03-03T11:29:47.023280Z"
    }
   },
   "outputs": [],
   "source": [
    "trt_fp16_outputs = [output[0] for output in trt_fp16_outputs]\n",
    "trt_int8_outputs = [output[0] for output in trt_int8_outputs]\n",
    "pytorch_cpu_outputs_numpy = [tensor.detach().numpy() for tensor in pytorch_cpu_outputs]\n",
    "pytorch_gpu_outputs_numpy = [tensor.cpu().detach().numpy() for tensor in pytorch_gpu_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c11c36df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:47.032758Z",
     "start_time": "2022-03-03T11:29:47.027949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7059348e-07"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mean_discrepency(pytorch_cpu_outputs_numpy, pytorch_gpu_outputs_numpy, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9a15caf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:47.037249Z",
     "start_time": "2022-03-03T11:29:47.033731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005077717"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mean_discrepency(trt_int8_outputs, trt_fp16_outputs, 1e-3)\n",
    "# Small differencies between TRT fp16 model outputs and TRT int8 model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37f534a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:47.041642Z",
     "start_time": "2022-03-03T11:29:47.038229Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0006866513"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mean_discrepency(pytorch_cpu_outputs_numpy, trt_fp16_outputs, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067a3f66",
   "metadata": {},
   "source": [
    "## Latency comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8f12fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:47.045517Z",
     "start_time": "2022-03-03T11:29:47.042615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3879566197283566"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_latency_pytorch_cpu_secs = np.mean(pytorch_cpu_time_buffer)\n",
    "mean_latency_pytorch_cpu_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96ecb1d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:47.049177Z",
     "start_time": "2022-03-03T11:29:47.046472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05832288501784205"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_latency_pytorch_gpu_secs = np.mean(pytorch_gpu_time_buffer)\n",
    "mean_latency_pytorch_gpu_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a49e8d2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:47.052916Z",
     "start_time": "2022-03-03T11:29:47.050174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011639913546387106"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_latency_trt_fp16_secs = np.mean(trt_fp16_time_buffer)\n",
    "mean_latency_trt_fp16_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62268159",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-03T11:29:47.056631Z",
     "start_time": "2022-03-03T11:29:47.053896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01163815296953544"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_latency_trt_int8_secs = np.mean(trt_int8_time_buffer)\n",
    "mean_latency_trt_int8_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298bb06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "tensorrt_env_2",
   "language": "python",
   "name": "tensorrt_env_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
